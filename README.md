# ChartMuseum


## Overview

**ChartMuseum** is a chart question answering benchmark designed to evaluate reasoning capabilities of large vision-language models
(LVLMs) over real-world chart images. The benchmark consists of 1162 *(image, question, short answer)* tuples and exclusively targets at questions that requires non-trivial text and visual reasoning skills. The dataset is collectively annotated by a team of 13 researchers in computer science.


## Installation

To set up the environment for accessing the dataset and evaluating model performance, follow these steps:

```bash
# Create and activate a new conda environment
conda create -n chartmuseum python=3.11
conda activate chartmuseum

# Clone the repository
git clone https://github.com/username/chartmuseum.git
cd chartmuseum

# Install dependencies
pip install -r requirements.txt
```


## Benchmark Access

Our benchmark is available at Hugging Face ğŸ¤—. More benchmark details can be found [here](https://huggingface.co/datasets/lytang/ChartMuseum).


```python
from datasets import load_dataset
dataset = load_dataset("lytang/ChartMuseum")
```

The benchmark contains the following fields:
|Field| Description |
|--|--|
|image| an image where the question is based on|
|question| a question on an image|
|answer| an answer to a question|
|reasoning_type| the reasoning skill that is primarily required to answer the question - *text*, *visual/text*, *synthesis*, *visual*|
|source| the website where we collect the image |
|hash| a unique identifier for the example |

The question answering prompt we used for all models is included in [prompt.py](prompt.py).

## Output Evaluation Instruction

Once your model predictions on the benchmark are ready, we provide an evaluation script to compute the accuracy of model answers, which is very simple to run. We use `gpt-4.1-mini-2025-04-14` checkpoint from OpenAI as the LLM-as-a-Judge model for our benchmark. Make sure to set up your OpenAI API key in your environment variables.


```python
export OPENAI_API_KEY=your_api_key_here
python evaluate.py --prediction_path /path/to/predictions.json --split dev/test
```

Optionally, you can specify `--save_dir /path/to/save_dir` to save the evaluation results. The evaluation script will extract the short answers from predictions and compare them with the ground truth answers. The evaluation script will output:
* The cost and time spent on the evaluation. The estimated cost is \$0.03 and \$0.16 on dev and test set, respectively. The evaluation typically takes around 5s for the dev set and 12s for the test set.
* The accuracy of the model on the dev/test set.


The prediction file should contain a list of strings, where each string correpsonds to an answer of a question in the dataset. The order of the answers should match the order of the questions in the dataset. Note that we require each string to contain an answer wrapped in the `<answer></answer>` tags. As our evaluation script will automatically extract the answer from the string.

```
[
    "...<answer>predicted short answer 1</answer>...",
    "...<answer>predicted short answer 2</answer>...",
    ...
]
```

### Evaluation Demo

To demonstrate the evaluation process, we'll use outputs from Claude-3.7-Sonnet on the dev set. The example below shows how to run the evaluation with our provided [example_outputs](example_outputs) (both formatting styles are supported):

```python
export OPENAI_API_KEY=your_api_key_here
python evaluate.py \
    --prediction_path example_outputs/claude-3-7-sonnet-20250219-dev-full-output.json \
    --split dev
```

<details>
<summary>Expected Output (shortened)</summary>

```
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 100% â€¢ Time Elapsed 0:00:05 â€¢ Time Remaining 0:00:00                                     
Requests: Total: 162 â€¢ Cached: 0âœ“ â€¢ Success: 162âœ“ â€¢ Failed: 0âœ— â€¢ In Progress: 0â‹¯ â€¢ Req/min: 1703.1 â€¢ Res/min: 1703.1                                                    
                Final Curator Statistics                
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Section/Metric             â”‚ Value                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Model                      â”‚                         â”‚
â”‚ Name                       â”‚ gpt-4.1-mini-2025-04-14 â”‚
â”‚ Rate Limit (RPM)           â”‚ 12000                   â”‚
â”‚ Rate Limit (TPM)           â”‚ 4000000                 â”‚
â”‚ Requests                   â”‚                         â”‚
â”‚ Total Processed            â”‚ 162                     â”‚
â”‚ Successful                 â”‚ 162                     â”‚
â”‚ Failed                     â”‚ 0                       â”‚
â”‚ Tokens                     â”‚                         â”‚
â”‚ Total Tokens Used          â”‚ 0                       â”‚
â”‚ Total Input Tokens         â”‚ 0                       â”‚
â”‚ Total Output Tokens        â”‚ 0                       â”‚
â”‚ Average Tokens per Request â”‚ 0                       â”‚
â”‚ Average Input Tokens       â”‚ 0                       â”‚
â”‚ Average Output Tokens      â”‚ 0                       â”‚
â”‚ Costs                      â”‚                         â”‚
â”‚ Total Cost                 â”‚ $0.027                  â”‚
â”‚ Average Cost per Request   â”‚ $0.000                  â”‚
â”‚ Input Cost per 1M Tokens   â”‚ $0.400                  â”‚
â”‚ Output Cost per 1M Tokens  â”‚ $1.600                  â”‚
â”‚ Performance                â”‚                         â”‚
â”‚ Total Time                 â”‚ 5.71s                   â”‚
â”‚ Average Time per Request   â”‚ 0.04s                   â”‚
â”‚ Requests per Minute        â”‚ 1700.9                  â”‚
â”‚ Responses per Minute       â”‚ 1700.9                  â”‚
â”‚ Max Concurrent Requests    â”‚ 35                      â”‚
â”‚ Input Tokens per Minute    â”‚ 0.0                     â”‚
â”‚ Output Tokens per Minute   â”‚ 0.0                     â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
Final Accuracy: 0.6296
```
</details>

The inference time with our evaluation script takes 5s and 12s for the dev and test set, respectively.


## License

Our benchmark is licensed under [CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/deed.en). Copyright of all included charts is retained by their original authors and sources.