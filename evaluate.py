from bespokelabs import curator
import re
import argparse
import logging
import pandas as pd
import json
import os
from datasets import Dataset, load_dataset
from prompt import COMPARE_ANSWER_PROMPT


class AnswerCompareGenerator(curator.LLM):

    '''
    This class is used to compare the answer generated by the model with the ground truth answer.
    '''

    def prompt(self, input: dict) -> str:
        return COMPARE_ANSWER_PROMPT.replace('[QUESTION]', input['question']).replace('[ANSWER1]', input['answer1']).replace('[ANSWER2]', input['answer2'])

    def parse(self, input: dict, response) -> dict:
        input['equal_answer'] = 1 if 'yes' in response.lower() else 0
        return input


class AnswerEvaluator:

    def __init__(
        self,
        split: str = None,  # 'dev' or 'test'
        model_name: str = 'gpt-4.1-mini-2025-04-14',
        working_dir: str = './cache'  # this will be automatically created if not exist
    ):
        self.model_name = model_name
        self.working_dir = working_dir
        self.split = split
        self._init_llm_generator()

    def _init_llm_generator(self):
        default_config = {
            'model_name': self.model_name,
            'generation_params': {'temperature': 0},
            'backend_params': {
                'max_requests_per_minute': 12000, # lower the rate limit to avoid hitting the API limit (if necessary)
                'max_tokens_per_minute': 4000000,
                'seconds_to_pause_on_rate_limit': 15.0
            }
        }
        self.compare_answer_generator = AnswerCompareGenerator(**default_config)


    def extract_answer(self, text):
        '''
        Extract the answer from the generated text.
        '''
        answer_pattern = r'<answer>(.*?)</answer>'    
        answer_match = re.search(answer_pattern, text + '</answer>', re.DOTALL) # add the tag for handling bad formatted answers
        return answer_match.group(1).strip() if answer_match else ""


    def construct_dataset_for_answer_compare(self, prediction_path):

        '''
        Construct the dataset for answer comparison.
        The dataset contains the question, ground truth answer, generated answer, and the generated text.
        The generated text is used to extract the answer.
        The dataset is used to compare the generated answer with the ground truth answer.
        '''

        benchmark = pd.DataFrame(load_dataset("lytang/ChartMuseum")[self.split])
        questions = benchmark.question.values.tolist()
        ground_truths = benchmark.answer.values.astype(str).tolist()

        with open(prediction_path, 'r') as file:
            predictions = json.load(file)
        dataset = Dataset.from_dict({"predictions": predictions})

        pred_answers = dataset.map(
            lambda x: {"answer": str(self.extract_answer(x["predictions"]))},
            num_proc=os.cpu_count(),
            remove_columns=dataset.column_names,
            desc="Extracting answers from outputs" 
        )["answer"]

        dataset = Dataset.from_pandas(pd.DataFrame({'question': questions, 'answer1': ground_truths, 'answer2': pred_answers, 'pred_cot': predictions}))

        return benchmark, dataset


    def evaluate(self, prediction_path):

        '''
        Evaluate the accuracy of the generated answers against the ground truth answers.
        '''

        benchmark, dataset_for_compare = self.construct_dataset_for_answer_compare(prediction_path)

        equal_answer = self.compare_answer_generator(dataset_for_compare, working_dir=self.working_dir)['equal_answer']
        accuracy = sum(equal_answer) / len(equal_answer)

        benchmark['pred_answer'] = dataset_for_compare['answer2']
        benchmark['equal_answer'] = equal_answer

        return {
            'benchmark': benchmark,
            'accuracy': accuracy,
        }


if __name__ == '__main__':

    parser = argparse.ArgumentParser()
    parser.add_argument('--prediction_path', type=str, help='Path to the predictions file (JSON)', required=True)
    parser.add_argument('--split', default=None, type=str, help='Split of the dataset to evaluate (dev/test)')
    parser.add_argument('--save_dir', default=None, type=str, help='Path to save the correctness of the predictions')
    args = parser.parse_args()

    logging.basicConfig(
        level=logging.INFO,  
        format='%(asctime)s [%(levelname)s] %(message)s',
        handlers=[
            logging.StreamHandler()
        ]
    )

    logging.getLogger("curator.bespokelabs").setLevel(logging.INFO)
    logging.getLogger().setLevel(logging.INFO)  

    if args.save_dir:
        assert not os.path.exists(args.save_dir), f'File already exists at {args.save_dir}'

    evaluator = AnswerEvaluator(split=args.split, model_name='gpt-4.1-mini-2025-04-14')
    results = evaluator.evaluate(args.prediction_path)
    print(f"Final Accuracy: {results['accuracy']:.4f}")

    if args.save_dir:
        with open(args.save_dir, 'w') as file:
            json.dump(results['benchmark']['equal_answer'].tolist(), file)

