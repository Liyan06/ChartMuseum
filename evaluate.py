from bespokelabs import curator
import re
import argparse
import logging
import pandas as pd
import json
import os
from datasets import Dataset, load_dataset


COMPARE_ANSWER_PROMPT = """You are provided with a question and two answers. Please determine if these answers are equivalent. Follow these guidelines:

1. Numerical Comparison:
   - For decimal numbers, consider them as equivalent if their relative difference is sufficiently small. 
   For example, the following pairs are equivalent:
    - 32.35 and 32.34
    - 90.05 and 90.00
    - 83.3% and 83.2%
    - 31 and 31%
   The following pairs are not equivalent:
   - 32.35 and 35.25
   - 90.05 and 91.05
   - 83.3% and 45.2%

   Note that if the question asks for years or dates, please do the exact match with no error tolerance.

2. Unit Handling:
   - If only one answer includes units (e.g. '$', '%', '-', etc.), ignore the units and compare only the numerical values
   For example, the following pairs are equivalent:
   - 305 million and 305 million square meters
   - 0.75 and 0.75%
   - 0.6 and 60%
   - $80 and 80
   The following pairs are not equivalent:
   - 305 million and 200 million square meters
   - 0.75 and 0.90%

3. Text Comparison:
   - Ignore differences in capitalization
   - Treat mathematical expressions in different but equivalent forms as the same (e.g., "2+3" = "5")

Question: [QUESTION]
Answer 1: [ANSWER1]
Answer 2: [ANSWER2]

Please respond with:
- "Yes" if the answers are equivalent
- "No" if the answers are different"""


class AnswerCompareGenerator(curator.LLM):

    '''
    This class is used to compare the answer generated by the model with the ground truth answer.
    '''

    def prompt(self, input: dict) -> str:
        return COMPARE_ANSWER_PROMPT.replace('[QUESTION]', input['question']).replace('[ANSWER1]', input['answer1']).replace('[ANSWER2]', input['answer2'])

    def parse(self, input: dict, response) -> dict:
        input['equal_answer'] = 1 if 'yes' in response.lower() else 0
        return input


class AnswerEvaluator:

    def __init__(
        self,
        split: str = None,  # 'dev' or 'test'
        model_name: str = 'gpt-4.1-mini-2025-04-14',
        working_dir: str = './cache'  # this will be automatically created if not exist
    ):
        self.model_name = model_name
        self.working_dir = working_dir
        self.split = split
        self._init_llm_generator()

    def _init_llm_generator(self):
        default_config = {
            'model_name': self.model_name,
            'generation_params': {'temperature': 0},
            'backend_params': {
                'max_requests_per_minute': 12000, # lower the rate limit to avoid hitting the API limit (if necessary)
                'max_tokens_per_minute': 4000000,
                'seconds_to_pause_on_rate_limit': 15.0
            }
        }
        self.compare_answer_generator = AnswerCompareGenerator(**default_config)


    def extract_answer(self, text):
        '''
        Extract the answer from the generated text.
        '''
        answer_pattern = r'<answer>(.*?)</answer>'    
        answer_match = re.search(answer_pattern, text + '</answer>', re.DOTALL) # add the tag for handling bad formatted answers
        return answer_match.group(1).strip() if answer_match else ""


    def construct_dataset_for_answer_compare(self, prediction_path):

        '''
        Construct the dataset for answer comparison.
        The dataset contains the question, ground truth answer, generated answer, and the generated text.
        The generated text is used to extract the answer.
        The dataset is used to compare the generated answer with the ground truth answer.
        '''

        benchmark = pd.DataFrame(load_dataset("lytang/ChartMuseum")[self.split])
        questions = benchmark.question.values.tolist()
        ground_truths = benchmark.answer.values.astype(str).tolist()

        with open(prediction_path, 'r') as file:
            predictions = json.load(file)
        dataset = Dataset.from_dict({"predictions": predictions})

        pred_answers = dataset.map(
            lambda x: {"answer": str(self.extract_answer(x["predictions"]))},
            num_proc=os.cpu_count(),
            remove_columns=dataset.column_names,
            desc="Extracting answers from outputs" 
        )["answer"]

        dataset = Dataset.from_pandas(pd.DataFrame({'question': questions, 'answer1': ground_truths, 'answer2': pred_answers, 'pred_cot': predictions}))

        return benchmark, dataset


    def evaluate(self, prediction_path):

        '''
        Evaluate the accuracy of the generated answers against the ground truth answers.
        '''

        benchmark, dataset_for_compare = self.construct_dataset_for_answer_compare(prediction_path)

        equal_answer = self.compare_answer_generator(dataset_for_compare, working_dir=self.working_dir)['equal_answer']
        accuracy = sum(equal_answer) / len(equal_answer)

        benchmark['pred_answer'] = dataset_for_compare['answer2']
        benchmark['equal_answer'] = equal_answer

        return {
            'benchmark': benchmark,
            'accuracy': accuracy,
        }


if __name__ == '__main__':

    parser = argparse.ArgumentParser()
    parser.add_argument('--prediction_path', type=str, help='Path to the predictions file (JSON)', required=True)
    parser.add_argument('--split', default=None, type=str, help='Split of the dataset to evaluate (dev/test)')
    parser.add_argument('--save_dir', default=None, type=str, help='Path to save the correctness of the predictions')
    args = parser.parse_args()

    logging.basicConfig(
        level=logging.INFO,  
        format='%(asctime)s [%(levelname)s] %(message)s',
        handlers=[
            logging.StreamHandler()
        ]
    )

    logging.getLogger("curator.bespokelabs").setLevel(logging.INFO)
    logging.getLogger().setLevel(logging.INFO)  

    if args.save_dir:
        assert not os.path.exists(args.save_dir), f'File already exists at {args.save_dir}'

    evaluator = AnswerEvaluator(split=args.split, model_name='gpt-4.1-mini-2025-04-14')
    results = evaluator.evaluate(args.prediction_path)
    print(f"Final Accuracy: {results['accuracy']:.4f}")

    if args.save_dir:
        with open(args.save_dir, 'w') as file:
            json.dump(results['benchmark']['equal_answer'].tolist(), file)

